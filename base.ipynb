{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--batch_size', type=int, default=300)\n",
    "parser.add_argument('--train_epoch', type=int, default=5)\n",
    "parser.add_argument('--lr', type=float, default=1e-3, help='learning rate hyperparameter')\n",
    "parser.add_argument('--verbose', type=int, default=10)\n",
    "parser.add_argument('--data_path', type=str, default='D:/researches/codes/1.csv')\n",
    "args = parser.parse_known_args()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_float(input):\n",
    "    if input == '':\n",
    "        return -1\n",
    "    else:\n",
    "        return float(input)\n",
    "\n",
    "def formator(line):\n",
    "    result = []\n",
    "    result.append(to_float(line['satv_use']))\n",
    "    result.append(to_float(line['satm_use']))\n",
    "    result.append(to_float(line['satcomp_use']))\n",
    "    result.append(to_float(line['act_eng']))\n",
    "    result.append(to_float(line['act_math']))\n",
    "    result.append(to_float(line['act_read']))\n",
    "    result.append(to_float(line['act_sci']))\n",
    "    result.append(to_float(line['act_comp']))\n",
    "    result.append(to_float(line['nc1']))\n",
    "    result.append(to_float(line['nc2']))\n",
    "    result.append(to_float(line['nc3']))\n",
    "    result.append(to_float(line['ncav']))\n",
    "    result.append(to_float(line['hs_gpa']))   \n",
    "    result.append(to_float(line['college_gpa']))\n",
    "    label = [0,0,0,0]\n",
    "    if line['vt_adm_dec'][:2] == 'Ad':\n",
    "        label[0] = 1\n",
    "    elif line['vt_adm_dec'][:2] == 'De':\n",
    "        label[1] = 1\n",
    "    elif line['vt_adm_dec'][:2] == 'Wa':\n",
    "        label[2] = 1\n",
    "    else:\n",
    "        label[3] = 1\n",
    "    return result, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"df2019.csv\", 'r', newline='', encoding='utf-8') as file:\n",
    "    data = []\n",
    "    label = []\n",
    "    reader = csv.DictReader(file)\n",
    "    for row in reader:\n",
    "        features, decision = formator(row)\n",
    "        data.append(features)\n",
    "        label.append(decision)\n",
    "    data = np.array(data)\n",
    "    data = (data - np.min(data,axis=0))/(np.max(data,axis=0) - np.min(data,axis=0) + 1e-6)\n",
    "    label = np.array(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = list(range(data.shape[0]))\n",
    "idx = np.linspace(0,data.shape[0]-1,data.shape[0],dtype=int)\n",
    "random.shuffle(idx)\n",
    "train_size = int(data.shape[0]*0.8)\n",
    "train_idx = idx[:train_size]\n",
    "test_idx = idx[train_size:]\n",
    "train_data = data[train_idx]\n",
    "test_data = data[test_idx]\n",
    "train_label = label[train_idx]\n",
    "test_label = label[test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Admission_Dataset(Dataset):\n",
    "    def __init__(self, data, label):\n",
    "        self.data = data\n",
    "        self.label = label\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index], self.label[index]\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "\n",
    "train_dataset = Admission_Dataset(train_data,train_label)\n",
    "test_dataset = Admission_Dataset(test_data,test_label)\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=args.batch_size, shuffle=False)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=args.batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model):\n",
    "    correct_count = 0\n",
    "    for batch in test_loader:\n",
    "        data, label = batch\n",
    "        preds = model(data.float())\n",
    "        preds = torch.argmax(preds, dim=1)\n",
    "        label = torch.argmax(label, dim=1)\n",
    "        correct_count += torch.sum(preds==label)\n",
    "    return correct_count/test_data.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), args.lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    for i in tqdm(range(args.train_epoch)):\n",
    "        for j, batch in enumerate(train_loader):\n",
    "            data, label = batch\n",
    "            preds = model(data.float())\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(preds, label.float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if j%args.verbose == 0:\n",
    "                print('train epoch {} epoch {} loss: {}'.format(i,j,loss.item()))\n",
    "        print('train epoch {} classification accuracy: {}:'.format(i,test(model)))\n",
    "\n",
    "def save_rep(model):\n",
    "    model.save_rep(torch.tensor(data).float())\n",
    "    np.save('label.npy', label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, latent_dim = 64, input_length=14, class_num=4):\n",
    "        super().__init__()\n",
    "        self.lin1 = nn.Linear(input_length,latent_dim)\n",
    "        self.lin2 = nn.Linear(latent_dim,latent_dim)\n",
    "        self.lin3 = nn.Linear(latent_dim,class_num)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.lin1(x)\n",
    "        x = torch.nn.GELU()(x)\n",
    "        x = self.lin2(x)\n",
    "        x = torch.nn.GELU()(x)\n",
    "        x = self.lin3(x)\n",
    "        return x\n",
    "    \n",
    "    def save_rep(self, x):\n",
    "        np.save('original_rep.npy', x.detach().numpy())\n",
    "        x = self.lin1(x)\n",
    "        np.save('layer1_rep.npy', x.detach().numpy())\n",
    "        x = torch.nn.GELU()(x)\n",
    "        x = self.lin2(x)\n",
    "        np.save('layer2_rep.npy', x.detach().numpy())\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train epoch 0 epoch 0 loss: 1.3423652648925781\n",
      "train epoch 0 epoch 10 loss: 1.2181541919708252\n",
      "train epoch 0 epoch 20 loss: 1.0684549808502197\n",
      "train epoch 0 epoch 30 loss: 0.9459431171417236\n",
      "train epoch 0 epoch 40 loss: 1.0208693742752075\n",
      "train epoch 0 epoch 50 loss: 0.8968073725700378\n",
      "train epoch 0 epoch 60 loss: 0.9104130864143372\n",
      "train epoch 0 epoch 70 loss: 0.9260752201080322\n",
      "train epoch 0 epoch 80 loss: 0.8536872267723083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [00:00<00:01,  3.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train epoch 0 epoch 90 loss: 0.9041756391525269\n",
      "train epoch 0 classification accuracy: 0.6908879280090332:\n",
      "train epoch 1 epoch 0 loss: 0.9103702902793884\n",
      "train epoch 1 epoch 10 loss: 0.8606123924255371\n",
      "train epoch 1 epoch 20 loss: 0.8015207052230835\n",
      "train epoch 1 epoch 30 loss: 0.8587706685066223\n",
      "train epoch 1 epoch 40 loss: 0.9454024434089661\n",
      "train epoch 1 epoch 50 loss: 0.8378568291664124\n",
      "train epoch 1 epoch 60 loss: 0.8629657030105591\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2/5 [00:00<00:00,  3.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train epoch 1 epoch 70 loss: 0.8810014128684998\n",
      "train epoch 1 epoch 80 loss: 0.8125078678131104\n",
      "train epoch 1 epoch 90 loss: 0.8638734817504883\n",
      "train epoch 1 classification accuracy: 0.68885338306427:\n",
      "train epoch 2 epoch 0 loss: 0.8336718082427979\n",
      "train epoch 2 epoch 10 loss: 0.779705822467804\n",
      "train epoch 2 epoch 20 loss: 0.7451146245002747\n",
      "train epoch 2 epoch 30 loss: 0.7965748310089111\n",
      "train epoch 2 epoch 40 loss: 0.871676504611969\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3/5 [00:00<00:00,  3.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train epoch 2 epoch 50 loss: 0.7683835625648499\n",
      "train epoch 2 epoch 60 loss: 0.8257764577865601\n",
      "train epoch 2 epoch 70 loss: 0.8513416051864624\n",
      "train epoch 2 epoch 80 loss: 0.7946775555610657\n",
      "train epoch 2 epoch 90 loss: 0.8348208069801331\n",
      "train epoch 2 classification accuracy: 0.7035314440727234:\n",
      "train epoch 3 epoch 0 loss: 0.7990593314170837\n",
      "train epoch 3 epoch 10 loss: 0.7425222992897034\n",
      "train epoch 3 epoch 20 loss: 0.729369580745697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 4/5 [00:01<00:00,  3.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train epoch 3 epoch 30 loss: 0.7732084393501282\n",
      "train epoch 3 epoch 40 loss: 0.8468686938285828\n",
      "train epoch 3 epoch 50 loss: 0.7529251575469971\n",
      "train epoch 3 epoch 60 loss: 0.8167449831962585\n",
      "train epoch 3 epoch 70 loss: 0.8371982574462891\n",
      "train epoch 3 epoch 80 loss: 0.776539146900177\n",
      "train epoch 3 epoch 90 loss: 0.8226838111877441\n",
      "train epoch 3 classification accuracy: 0.7073099613189697:\n",
      "train epoch 4 epoch 0 loss: 0.7863752245903015\n",
      "train epoch 4 epoch 10 loss: 0.7287291288375854\n",
      "train epoch 4 epoch 20 loss: 0.7183182239532471\n",
      "train epoch 4 epoch 30 loss: 0.7602121829986572\n",
      "train epoch 4 epoch 40 loss: 0.8301041722297668\n",
      "train epoch 4 epoch 50 loss: 0.7440239191055298\n",
      "train epoch 4 epoch 60 loss: 0.8117803335189819\n",
      "train epoch 4 epoch 70 loss: 0.8269037008285522\n",
      "train epoch 4 epoch 80 loss: 0.7614269256591797\n",
      "train epoch 4 epoch 90 loss: 0.8121156692504883\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:01<00:00,  3.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train epoch 4 classification accuracy: 0.7084726095199585:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = MLP()\n",
    "train(model)\n",
    "save_rep(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs5805",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
